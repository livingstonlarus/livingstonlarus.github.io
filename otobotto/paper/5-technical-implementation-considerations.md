## 5. Technical Implementation Considerations

*(Section 5 details practical implementation aspects, such as how to containerize the agents, orchestrate their parallel execution, and interface with development tools. For brevity, we summarize key points.)*

Ōtobotto’s prototype is implemented with each agent running in an isolated container environment (Docker) with access to a shared filesystem (for the repository and operational memory) and network (for tool APIs). We used a combination of Python and Node.js to script agent behaviors where needed (especially for DevOps tasks). The orchestrator and agents communicate through files and a lightweight message queue.

To make the multi-agent system feasible to run, we employed a **phased implementation** approach. We started by focusing on a single language (TypeScript for the target project’s code) and got core development and testing agents working with a basic orchestrator loop. Then we incrementally added the documentation agent, UI agent, etc., and introduced more complex orchestrator behaviors. This incremental build-out allowed testing each component in isolation before dealing with full concurrency.

One practical challenge is **token cost and API rate limits** when using large LLMs. We mitigated this via adaptive context management – agents summarize or compress context when near limits, and by using **OpenRouter** (an API aggregator) for model access which allowed higher rate limits for research purposes. In particular, for our orchestrator we used **Anthropic’s Claude Instant v3.7 (“Sonnet” mode)** which offers a ~200k token context window; we accessed it via OpenRouter to avoid hitting single-user rate caps. For coding, we used **Google Gemini 2 Pro (experimental)** which boasts a 2 million token context; this was accessed directly through a research API (since OpenRouter did not yet proxy the 2M context version). These model choices were strategic: Claude’s reasoning and planning strengths, combined with Gemini’s expansive context and coding ability, provided a balance of capabilities. Using two different vendors also demonstrated Otobotto’s model-agnostic flexibility – the agents communicated via files, not via proprietary features, so any mix of models can be substituted as long as they follow the protocol.

We had to implement some **workarounds** for model quirks. For instance, “Sonnet” mode generates a chain-of-thought that can be verbose; we filtered some of it for brevity in operational memory to avoid overwhelming other agents with the orchestrator’s raw thoughts. For Gemini, we had to chunk the context to ensure important parts (like current file contents and relevant tests) stayed within its attention despite the huge window (which, while large, cannot always fully utilize 2M tokens effectively due to API constraints). These experiences informed our design: in a deployed Otobotto, one might use open-source models fine-tuned for each role to avoid such constraints, or run large models on-prem for privacy and cost control.

In terms of tool integration, each agent was given access to specific tool APIs related to its role. For example, Dev agents could call a code compiler and linter, Testing agents could invoke a test runner and coverage tool, Documentation agents could use a Markdown/PDF generator, and so forth. We used a **message-passing file system** (inspired by Runic’s design): agents write intents or results to files (e.g., `test_results.json`) that other agents watch for. This decouples the agents and avoids requiring direct inter-process communication, which made it easier to run multiple instances in parallel.

Security was considered in a basic way: agents are constrained by Docker and a permission layer (for example, only Security agents have credentials to pull from the vulnerability database; only DevOps agents can deploy to the cloud account). This principle of least privilege will be important to mitigate any errant behavior by an agent.

Finally, we open-sourced the core framework of our prototype to encourage community contributions, and to allow others to experiment with plugging in different models or extending it for other tech stacks. We envision that an open ecosystem could emerge around Otobotto, similar to how people contribute plugins or tools to other AI frameworks, which would accelerate development and adoption.

*(The above is a condensed summary of technical implementation details. In an actual paper, more specifics on infrastructure, model prompting strategies, and performance optimizations would be provided.)*

