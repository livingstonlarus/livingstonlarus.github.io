## 7. Preliminary Evaluation and Future Considerations

As Ōtobotto is a comprehensive framework, evaluating its effectiveness requires examining multiple dimensions. In this section, we outline how we plan to empirically assess Ōtobotto once implemented, and we also describe a preliminary experiment conducted with a simplified version of the system to gain early insights. We then discuss practical considerations for realizing the full system in real-world settings.

### 7.1 Key Evaluation Dimensions

Once Ōtobotto (or a prototype thereof) is operational, we will conduct thorough evaluations focusing on several critical dimensions:

-   **Development Efficiency:** We will measure how Ōtobotto affects development speed and resource utilization compared to conventional workflows or other AI-assisted methods. Key metrics include **time to implement a set of features**, the effective throughput of completing tasks, and the compute cost (e.g., total model inference time or token consumption) for a given scope of work. We anticipate that parallel agent activity and automation will **reduce development time** significantly. For example, we plan to compare two teams building the same module – one using Ōtobotto extensively, and one composed of human developers using their standard tools (including modern AI coding assistants like GitHub Copilot, to be fair since most human engineers now work with AI help). We will measure completion time, person-hours expended, and perhaps costs. A successful outcome would be Otobotto achieving similar or faster delivery compared to the human team, at a fraction of the person-hours. This would demonstrate a productivity boost even accounting for humans using AI as baseline.

-   **Code Quality and Accuracy:** Using the integrated verification framework, we can quantify the quality of the output. Metrics include the number of defects discovered (during testing or in production), test coverage levels achieved, code maintainability scores (using static analysis for complexity, lint rules, etc.), and adherence to best practices. We expect Ōtobotto’s test-driven approach and multi-agent reviews to yield **high test coverage and fewer post-release bugs** than traditional development. We will also conduct code reviews by experienced human engineers on Ōtobotto-generated code vs. human-written code for the same functionality – to assess readability, clarity, and maintainability. If the AI code is on par or better on these qualitative aspects, that’s a strong positive indicator. Accuracy in meeting specifications can be measured by how many requirement acceptance criteria are met without iteration.

-   **Maintainability and Evolvability:** Over longer projects, does Otobotto produce code that is easier or harder to maintain? We will look at factors like modularity (are the components well-decoupled), documentation completeness (does every module have up-to-date docs), and whether adding new features later remains straightforward. One possible metric is the effort required for a human developer unfamiliar with the project to come in and successfully make a change – we could simulate this by handing the resulting codebase to a developer and timing how long it takes them to implement a new requirement. If Ōtobotto’s structured approach produces cleaner architecture, maintainability should be improved or at least not worse than a human team’s result.

-   **Human-AI Collaboration Effectiveness:** Through surveys and observational studies, we will evaluate how well the progressive autonomy and HITL strategies work in practice. Key questions: Do human developers feel their workload is reduced? Do they trust the AI’s decisions? Are there instances of confusion or miscommunication (for example, the AI misinterpreting human feedback or vice versa)? We will log how many times humans had to intervene or override AI decisions, and whether those interventions decrease over time – indicating learning and trust building. Ideally, as the project progresses, the human oversight becomes more about high-level guidance and less about redoing AI work. We’ll also measure “collaborative efficiency” – how efficiently the AI swarm and humans together can resolve issues. This can be measured in something like the average turnaround time for an AI question to get answered by a human and incorporated, or vice versa. If the strategies we implemented (batching, clear explanation requests) are effective, this turnaround should be short and not a bottleneck.

-   **Scalability and Parallelism:** We will stress-test Ōtobotto on projects of increasing complexity to see how it scales. This involves scaling in two dimensions: project size (e.g., can it handle a codebase of millions of lines across dozens of microservices?) and team size (e.g., what happens if we run 50 agents concurrently – does coordination overhead remain manageable?). Metrics here include how development time scales with additional agents (ideally sub-linearly, indicating benefit from parallelism) and how system resources (like token usage) grow with project size. We particularly monitor token consumption vs. project size, since one concern raised in literature is that naive multi-agent approaches might blow up token usage exponentially. Our memory hierarchy is intended to tame that by not needing to reload all context everywhere. We will evaluate system throughput (e.g., tasks completed per hour) as we add more agents; if adding agents yields near-linear speedup initially and then plateaus due to coordination overhead, that tells us the practical limits. We also watch for any breakdowns – e.g., with too many agents, do they start stepping on each other’s toes (like two agents making conflicting code changes)? If so, that indicates where improvements in the orchestrator or agent role definitions are needed.

-   **Benchmarking vs. Human Baselines:** Ultimately, to validate Ōtobotto, we must compare its performance against skilled human engineers (who are using state-of-the-art tools). We plan controlled experiments or hackathon-style challenges where, given the same requirements, one group uses Ōtobotto and another is a human team using tools like Copilot, automated tests, etc. We will compare outcomes: implementation time, number of bugs found, quality of final code, and possibly even external user feedback on the functionality if we can do a blind test (like have end-users use both versions of a small app). Success for Ōtobotto would mean it achieves comparable or better results in significantly less time or with fewer human resources. We recognize that human creativity and problem-solving is the gold standard, so an AI swarm doesn’t have to *beat* the best humans outright to be valuable – even if it comes close with far less human effort, that’s transformative for productivity.

These evaluations would likely be done on a mix of controlled tasks (to have clear ground truth and comparability) and real-world pilot projects with industry collaborators (to see effect in situ). The combination will give us both quantitative rigor and practical validation.

### 7.2 Implementation Considerations and Challenges

Building and deploying Ōtobotto in practice will come with challenges that we must address:

-   **Engineering Complexity:** As evident, Ōtobotto is complex: it intertwines orchestration, multiple AI models, development tool integration, and novel workflows. Implementing all components end-to-end is a substantial engineering project. Our approach is to implement and test incrementally. For instance, start with a narrower slice – maybe a subset focusing on backend code generation with one orchestrator and a few dev/test agents on a simple project – then gradually add more agents (documentation, DevOps, etc.) and scale up. This way, we can identify bottlenecks or failure modes early with less at stake. We also plan to leverage open-source contributions and existing frameworks. Rather than reinventing every wheel, we’ll integrate with frameworks like LangChain for the retrieval system or use existing multi-agent coordination libraries (if any prove suitable) as a foundation. Community involvement can be invited: for example, one could allow plugin agents that people develop for specialized tasks, which Ōtobotto could incorporate.

-   **Model Dependencies and Evolution:** The capabilities of Ōtobotto will partly rely on which LLMs are used. As new, more powerful models emerge (e.g., a next-gen GPT or future Google Gemini improvements), we’ll want to adapt. This is actually an opportunity for continuous improvement – the architecture is model-agnostic, so we can plug in upgrades as they come. However, it means our evaluation targets might shift (e.g., results with 2024 models vs 2025 models could differ just because the underlying models got better). We’ll clearly note the model versions in evaluations. There’s also a risk that certain model APIs change or become more restrictive/expensive. We’ve mitigated this by designing with the possibility to swap to open models – e.g., if a vendor API becomes too costly, one could fine-tune a local model on the project and use that for some agents. The modular design allows that flexibility.

-   **Enterprise Integration & Environment Compatibility:** To test Ōtobotto in a real enterprise, we must ensure it works with common enterprise stacks and tools. That means implementing connectors for systems like Jira (for reading requirements or posting updates), Jenkins or GitLab CI, artifact repositories, etc. In our pilot with a partner company, time will be spent setting up these integrations. We also must deploy Ōtobotto in a secure manner – likely within the company’s network if code privacy is a concern. Running everything on-prem (or in the company’s cloud) using either allowed APIs or self-hosted models is something we have to prepare for. This addresses security: companies will not be okay with proprietary code flowing to external services without permission. Our design where all major decisions and code remain in a local Git repo, and models can be self-hosted if needed, is aligned with this need. Success in an enterprise pilot would be measured by how smoothly Otobotto can slot into their dev environment and start contributing without causing disruptions or violating any policies.

-   **User Training and Onboarding:** Introducing Ōtobotto to a team requires training the team on how to work with it. There will be a learning curve – understanding the orchestrator’s output, knowing how to give effective feedback to AI agents, interpreting the rationale logs, etc. As part of a pilot, we’ll likely provide documentation or a short workshop for users. We will measure how long it takes for developers to get comfortable. Perhaps initially they might be skeptical or slow to trust it, but after a few cycles of positive interaction, that should improve. We’ll gather any recurring confusion points (e.g., maybe users aren’t sure how to intervene when the AI is wrong) and refine the UX and instructions. Our hypothesis is that developers will adapt quickly since we keep their tools the same (they still use Git, still run tests – just with an AI helping). Proving this is key to adoption; if it’s too hard to learn, organizations won’t bother.

-   **Impact on Team Dynamics:** We will also observe the qualitative impact on the human team. Does anyone feel threatened or, conversely, relieved by the presence of AI? Perhaps roles will shift – maybe a senior engineer becomes more of a mentor to the AI, reviewing and guiding it, rather than coding by hand. We will interview participants about how their job content changed and whether that’s positive. One concern might be that junior devs get fewer learning opportunities if the AI does the easy stuff; we need to ensure the system is used in a way that junior engineers can still learn (maybe by reading the AI’s output and rationale). If we find that, for example, too many approval tasks fall on one human, we might recommend distributing that (like have multiple humans in the loop for different domains). These qualitative measures help refine how Otobotto should be integrated. Ultimately, a successful integration is one where human team members feel *enhanced* by the AI, not replaced, and where they recognize that they can focus on more rewarding work (design, big-picture problem solving) while the AI handles repetitive grunt work.

-   **Robustness Testing:** We plan to push the system with edge cases: ambiguous requirements, sudden mid-project requirement changes, introduction of tricky bugs to see if the AI catches them, etc.. We want to see how well Ōtobotto can recover and adapt. Does it detect confusion and ask for clarification? If a requirement is changed, does it gracefully replan and refactor code as needed, or does it get tangled trying to reconcile new and old info? These stress tests will highlight where more intelligence is needed. For instance, an orchestrator might need a feature to explicitly handle requirement changes by marking what parts of the plan are obsolete and regenerating those. If we find issues, we’ll incorporate solutions (like more meta-cognitive checks for consistency). 

-   **A/B Testing Autonomy in Workflow:** In a longer enterprise trial, we could consider an A/B test where for some tasks we allow the AI more autonomy and for others we enforce more human control, and then compare outcomes. This would give statistical rigor to understanding how autonomy level affects productivity and quality. For example, randomly route half of the features to be AI-led (with minimal human interference) and half to be human-led (with AI in advisory role), then see differences in cycle time and error rates. This might require a sufficiently large project to gather data, but would be enlightening.

These considerations ensure our evaluation plan isn’t just about raw metrics in isolation but about validating Ōtobotto in realistic scenarios and iterating on its design for practicality. The outcome of evaluation will not only tell us how well it works, but also guide improvements and highlight which aspects yield the greatest benefits or pose the biggest challenges.

### 7.3 Preliminary Experiment: Runic-Based Prototype

To gain early insight into the feasibility of Ōtobotto’s approach, we conducted a preliminary experiment using **Runic**, a precursor framework to Ōtobotto, in a controlled environment. The goal was to observe multi-agent coordination in action on a non-trivial coding project and to identify practical challenges (especially around concurrency and context-sharing) before building the full system.

**Experimental Setup:** We set up a development environment using **VSCodium** (an open-source VS Code distribution) with the **Roo** VS Code extension for agent-assisted coding. The Roo extension provides an interface for multiple AI “agents” to operate within the editor, following instructions in special markdown files designated for orchestrator and specialist roles. While Roo is normally used for a single orchestrator and one assistant agent, we **hacked it to run multiple specialist agents in parallel**, effectively simulating a swarm. In practice, this meant launching multiple instances of the extension and coordinating them through shared files. The heavy lifting of multi-agent orchestration was handled by our Runic framework logic, not Roo itself – Roo simply served as the sandbox for each agent to read/write files. We configured **Runic v1.0** (which implements a basic orchestrator and multiple specialist agents with a file-based memory bank) to run within this IDE environment.

For the AI models, we utilized two state-of-the-art LLMs via API:

-   **Anthropic’s Claude Instant v3.7 (“Sonnet” mode)** as the Orchestrator agent. This model has an ~200k token context window and produces a chain-of-thought styled output (“Sonnet Thinking”) which we found useful for transparency. Using OpenRouter’s proxy, we bypassed strict rate limits, allowing the orchestrator to continuously generate plans and read/write large context from the shared files.

-   **Google Gemini 2 Pro (experimental)** as the Specialist coding agent(s). This model, with an up to 2M token context, was run in a “thinking” mode that outputs its reasoning steps while coding. We accessed Gemini’s API directly (since at the time OpenRouter did not support this model) to leverage the full context window. In practice, we limited context to around 100k tokens for performance, but the expanded window meant the coding agent could load multiple files and lengthy instructions simultaneously without issue.

These model choices reflect what was available as cutting-edge: Claude for coordination and planning, and Gemini for heavy-duty coding with vast context. We also aimed to test vendor-agnostic integration by using one model from Anthropic and one from Google. The orchestrator (Claude) would read the project instructions and break work into tasks, while one or more developer agents (Gemini instances) implemented code for those tasks. Communication between agents was facilitated by Roo/Runic’s mechanism: agents wrote to and read from specified markdown files serving as a **shared memory**, as per Runic’s design.

**Project Undertaken:** We selected a moderately complex project representative of an enterprise web application to push the limits of multi-agent parallelism. The project was a web platform with multiple components and integration points. In particular, it included the following concurrent engineering tracks:

-   **Account Management:** Implement user registration, login, profile management with role-based access control.
-   **Customer Support Module:** Include a support ticket system where customers can create tickets and support agents can respond.
-   **Email Notifications:** Integrate an email service to send notifications (welcome emails, password resets, ticket updates).
-   **Google Ads Integration:** Connect to the Google Ads API to pull advertising campaign data for analytics within our app.
-   **Internationalization (i18n):** Support multiple languages throughout the UI, with a mechanism to load locale files.
-   **Kubernetes Deployment Client:** Provide scripts or a module to help deploy the app on a Kubernetes cluster (generating YAML configs).
-   **QStash Integration:** Use the QStash service (a task queue API) to schedule background jobs for certain features (like sending emails or syncing ads data periodically).
-   **Subscription Management:** Implement a payment subscription system (with dummy integration to a payment gateway) to handle premium accounts.
-   **Testing & Monitoring:** Set up a monitoring dashboard and health check endpoints, plus comprehensive test suites.
-   **UI/UX Improvements:** Apply a design system to the front-end, ensuring a responsive and accessible interface.

This project was intentionally complex and multifaceted – far beyond a trivial “ToDo list” app – to simulate an enterprise scenario with many parallel workstreams. The idea was that an orchestrator could divide the project into these tracks, and multiple specialist agents could work simultaneously, each focusing on one track, with occasional synchronization where tracks intersect (for instance, the account management and subscription system both touch the user model).

We provided a high-level specification to the Orchestrator agent in an `orchestrator.md` file. This spec outlined each of the above tracks in a few sentences of requirements (e.g., “Account Management: users can sign up with email, verify email via a link, reset password, etc.” and tech stack suggestions like “use Node.js + Express for backend, MongoDB for database, send emails via SMTP”). We also specified some non-functional requirements: e.g., “the system should be able to handle 10k users”, and “follow OWASP security guidelines for the account system”, expecting the security agent to enforce that.

**Observations:**

1.  **Parallel Task Execution:** The Orchestrator (Claude) did successfully break down the project into parallel tasks. It created separate task lists for each major track (account, support, notifications, etc.) almost immediately, and spawned a specialist agent for each. At one point, we had 5 specialist agents coding in parallel on different modules. For example, one agent was building the authentication REST API while another simultaneously worked on the email sending module. This parallelism led to very fast initial development of skeleton features. We observed the orchestrator also assigning some agents to higher-priority tasks when needed – e.g., when the Ads Integration was lagging, it spun up an extra agent to assist on that track. Overall, the dynamic spawning and assignment validated our hypothesis about scalability: adding agents did linearly speed up development in this contained test. The entire first working prototype of the platform (all features minimally implemented) was achieved in about **1 hour**, largely autonomously. This would have easily taken a human team several hours or a day to scaffold. This efficiency showcases the potential of swarming over sequential development.

2.  **Context Sharing and Consistency:** A major challenge we noted was ensuring all agents had the necessary context as the codebase evolved. Since each agent ran with its own context window, sometimes an agent working on one part (say subscription billing) wasn’t aware of a related change made by another agent (like an update in account profile schema) until the orchestrator or a failing test alerted it. We saw the orchestrator playing a key role in broadcasting relevant changes – it periodically reminded agents of global schema and interfaces. We also leveraged the shared memory: agents would post summary notes after completing a task (e.g., “Added field X to User model”), which others could read. Despite this, a few inconsistencies slipped through initially (one agent used `username` field, another assumed `email` as key; one module named a database collection `Tickets` while another expected `SupportTickets`). The cross-verification process (tests and orchestrator review) caught these, and the agents corrected them in subsequent iterations. This highlighted the importance of a strong shared memory and perhaps a “contract” checking agent. In fact, based on this, we plan to introduce a schema validation agent in future runs. Encouragingly, when such mismatches were pointed out (either by failing integration tests or orchestrator prompts), the agents were able to reconcile them quickly – often the orchestrator would notice and prompt both relevant agents to synchronize on a common definition.

3.  **Specialist Agent Synergy:** Each specialist agent mostly focused on its domain (the code agents wrote code, the test agent wrote tests, etc.), but we observed emergent cooperation. For example, the testing agent didn’t wait idly; while dev agents coded, the testing agent began drafting integration tests. In one case, the testing agent’s scenario forced two dev agents to adjust their implementations to satisfy the test – effectively the test agent was driving design as intended by TDD. The documentation agent also was writing user guide snippets in parallel, and interestingly, one dev agent read the documentation draft and adjusted code to match the documented behavior. This shows the potential of having documentation as not just after-the-fact but as another source of truth during development. The orchestrator facilitated these synergies by keeping everyone updated in the shared markdown, but the agents themselves took initiative at times (the code agent referencing documentation is one example of unprompted but beneficial behavior).

4.  **Tool Orchestration via Roo/Runic:** Using the Roo extension as the sandbox, we encountered limitations. Roo wasn’t originally meant to handle simultaneous multi-agent edits – we sometimes ran into file lock/contention issues where two agents would write to the same file nearly at the same time. This required a simple locking mechanism we added in Runic (agents would announce intent to edit a file, orchestrator ensures one at a time). It slowed a couple of interactions but prevented conflicting writes. In a more robust deployment, a proper version control branching strategy would handle this. Another issue was context size – even though Gemini had a large window, pumping in the entire project state eventually became slow. We mitigated by letting agents decide which parts of the context to load (thanks to our memory system). For instance, the UI/UX agent didn’t need backend details in context. We consider this a positive outcome: it validated the need for and effectiveness of the hierarchical memory approach to keep context per agent relevant and slim.

5.  **Multi-agent Orchestration Feasibility:** Importantly, this experiment validated that orchestrating multiple AI agents concurrently on different aspects of a project is feasible and can yield correct, integrated results. By the end, we had a working web application where all the pieces built by different agents fit together (with some minor manual fixes on configuration). This was achieved without writing code ourselves – we acted only as the human overseer, answering a few questions (the orchestrator asked to clarify if the Ads API should be read-only or allow posting data; the security agent asked if we required two-factor auth for login – in those cases we gave guidance). The quality of the output was surprisingly solid given the short development time: the generated code followed good structure (the orchestrator seemed to enforce MVC separation, perhaps because our prompt context included a style guide), and all critical functionality was implemented. There were bugs, of course (some edge cases in ticket handling, and some parts were left stubbed due to time), but nothing fundamental that a second pass couldn’t resolve.

6.  **Lack of Quantitative Benchmark:** While this run was instructive, we did not do a side-by-side comparison with a human team due to limited time. So we cannot formally claim how many hours of human work this equated to. However, subjectively, accomplishing the described multi-module application in ~1 hour mostly autonomously is promising. We plan more rigorous benchmarking as described in Section 7.1, including measuring speed and accuracy versus experienced human developers using AI tools. Our anecdotal impression is that the AI swarm was **rapid**, but perhaps not yet as *thorough* or *innovative* as a human team might be in refining features. For instance, the AI implemented exactly what was asked, nothing more – whereas human developers might proactively add small improvements or question requirements. This indicates an area to improve: injecting a bit more creative or critical thinking agent to go beyond the spec.

7.  **Runic vs. Roo – Multi-agent Orchestration:** An important clarification from this experiment: the ability to orchestrate multiple agents in parallel came from our Runic framework, not from the Roo extension itself. Roo simply provided the interface for each agent to act in VSCode. Runic managed the file-based memory and scheduling of agents. This distinction matters because it shows that even existing developer tools can host Ōtobotto’s agents with some modifications – we’re not tied to a custom IDE. But it also highlighted that Roo’s native capabilities were exceeded; we were essentially pushing it beyond its intended use. This experience is feeding into the design of a custom orchestrator UI that will better visualize multi-agent activities and memory in real-time.

In summary, the preliminary experiment demonstrated the core concept of Ōtobotto: a dynamically assembled swarm of AI agents can collaborate to build a complex software system, and do so quickly. It also surfaced issues around synchronization, context management, and tool support, which have informed our ongoing development (for example, we are focusing on robust global state management and agent communication channels beyond just files). We also confirmed the importance of having integrated verification – whenever something went wrong, it was a test or a review agent that caught it, underscoring that our heavy emphasis on testing is well-placed.

This experiment, while small-scale, gives us confidence to proceed to more formal evaluations and eventually real-world pilot deployments. It showed that the vision of an “AI swarm engineer” is attainable with current technology, and it provided a blueprint of what to improve to make Ōtobotto truly production-ready.
